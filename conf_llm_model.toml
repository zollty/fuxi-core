# 设为 "auto" 会自动检测(会有警告)，也可手动设定为 "cuda","mps","cpu","xpu" 其中之一。
[llm]
device = "auto"

[embed]
device = "cuda"

[reranker]
top_n = 20
#max_length = 1024
#model_path = ""
#batch_size = 32

[reranker.model]
"bge-reranker-large" = "/ai/models/BAAI_bge-reranker-large"
"bge-reranker-base" = "BAAI/bge-reranker-base"
bce-reranker-base_v1 = "G:/50-TEMP/models/embed/bce-reranker-base_v1"


[llm.controller]
host = "0.0.0.0"
port = 21101
dispatch-method = "shortest_queue"

[llm_model.worker]
controller_address = "http://localhost:21101"
host = "0.0.0.0"
port = 21102
worker-address = "http://localhost:21102"
model-path = ""
model-names = ""
limit-worker-concurrency = 1024
num-gpus = 1
# conv-template = ""
# no-register
trust_remote_code = true
gpu_memory_utilization = 0.9

[llm.model_path]
"chatglm3-6b" = "/ai/models/chatglm3-6b"
"chatglm3-6b-32k" = "/ai/models/chatglm3-6b-32k"
"Qwen1.5-7B-Chat" = "/ai/models/Qwen1.5-7B-Chat"
"Qwen-7B-Chat" = "/ai/models/Qwen-7B-Chat"
"Qwen-14B-Chat" = "/ai/models/Qwen-14B-Chat-Int8"
"Qwen-1.8B-Chat" = "/ai/models/Qwen-1_8B-Chat-Int8"
"chatglm2-6b" = "/ai/models/chatglm2-6b"
"Chinese-Alpaca-2-7B" = "/ai/models/chinese-alpaca-2-7b-hf"
"Chinese-Alpaca-2-13B" = "/ai/models/chinese-alpaca-2-13b-16k-hf"
"Llama2-Chinese-13b-Chat" = "/ai/models/Llama2-Chinese-13b-Chat"
"Yi-34B-Chat-8bits" = "/ai/models/Yi-34B-Chat-8bits"
"Qwen-72B-Chat-Int8" = "/ai/models/Qwen-72B-Chat-Int8"


[llm.model_cfg."Qwen-7B-Chat"]# (4577 + 6089 + 7077=17743) mem <3*6GiB
"port" = 20002
"infer_turbo" = true
#"gpus" = "1,2,3,4", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 3 # 使用GPU的数量
"max_gpu_memory" = "12GiB" # 每个GPU占用的最大显存

[llm.model_cfg."Qwen1.5-7B-Chat"] # (4577 + 6089 + 7077=17743) mem <3*6GiB
"port" = 20003
"infer_turbo" = true
#"gpus" = "1,2,3,4", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 4 # 使用GPU的数量
"max_gpu_memory" = "12GiB" # 每个GPU占用的最大显存

[llm.model_cfg."chatglm2-6b"]
"port" = 20004
"infer_turbo" = true
#"gpus" = "0,1", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 5 # 使用GPU的数量
"max_gpu_memory" = "4GiB" # 每个GPU占用的最大显存

[llm.model_cfg."chatglm3-6b"] # (3867 + 4529 + 4507=12903) <13G mem <3*5GiB
"port" = 20005
"infer_turbo" = true
#"gpus" = "3", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 5 # 使用GPU的数量
"max_gpu_memory" = "4GiB" # 每个GPU占用的最大显存

[llm.model_cfg."chatglm3-6b-32k"]
"port" = 20006
"infer_turbo" = true
#"gpus" = "1,2,3,4,5", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 3 # 使用GPU的数量
"max_gpu_memory" = "16GiB" # 每个GPU占用的最大显存

[llm.model_cfg."Chinese-Alpaca-2-7B"]
"port" = 20007
"infer_turbo" = true
#"gpus" = "1", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 5 # 使用GPU的数量
"max_gpu_memory" = "5GiB" # 每个GPU占用的最大显存

[llm.model_cfg."Chinese-Alpaca-2-13B"]
"port" = 20008
"infer_turbo" = true
#"gpus" = "0", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 5 # 使用GPU的数量
"max_gpu_memory" = "6GiB" # 每个GPU占用的最大显存

[llm.model_cfg."Llama2-Chinese-13b-Chat"] # (8483 + 8779 + 8733=25995) <26G mem <3*9GiB
"port" = 20009
"infer_turbo" = true
#"gpus" = "1", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 5 # 使用GPU的数量
"max_gpu_memory" = "6GiB" # 每个GPU占用的最大显存

[llm.model_cfg."Qwen-14B-Chat"]#
"port" = 20010
"infer_turbo" = true
#"gpus" = "0", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 4 # 使用GPU的数量
"max_gpu_memory" = "10GiB" # 每个GPU占用的最大显存

[llm.model_cfg."Qwen-1.8B-Chat"]# 3gb
"port" = 20011
"infer_turbo" = true
#"gpus" = "3", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 3 # 使用GPU的数量
"max_gpu_memory" = "4GiB" # 每个GPU占用的最大显存

[llm.model_cfg."Yi-34B-Chat-8bits"] #
"port" = 20012
"infer_turbo" = true
#"gpus" = "0", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 5 # 使用GPU的数量
"max_gpu_memory" = "16GiB" # 每个GPU占用的最大显存

[llm.model_cfg."Qwen-72B-Chat-Int8"] #
"port" = 20013
"infer_turbo" = true
#"gpus" = "3", # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus" = 4 # 使用GPU的数量
"max_gpu_memory" = "22GiB" # 每个GPU占用的最大显存


[llm.openai_api_server]
host = "0.0.0.0"
port = 8880
controller_address = "http://localhost:21101"
api_keys = "EMPTY"
