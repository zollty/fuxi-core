llm:
  device: auto
  controller:
    host: 0.0.0.0
    port: 21001
    dispatch-method: shortest_queue
    #log_level: INFO
  openai_api_server:
    host: 127.0.0.1
    port: 20000
    controller_addr: http://localhost:21001
    api_keys: EMPTY
    #log_level: INFO
  worker:
    host: 127.0.0.1
    start_port: 21105
    #log_level: null
    #cross_domain: true
    base:
      controller_addr: http://localhost:21001
      #worker_address
      limit_worker_concurrency: 100
      conv_template: null
      no_register: false
      device: auto
      #num_gpus: 1
      max_gpu_memory: 10GB
      #model_model_path: null
      #model_names: null
      trust_remote_code: true
    plain: # see: fastchat\serve\model_worker.py
      debug: true
      #dtype: null
      #seed: null
      load_8bit: false
      stream_interval: 2
      cpu_offloading: false
      embed_in_truncate: false
#      awq:
#        awq_ckpt: null
#        awq_groupsize: -1
#        awq_wbits: 16
#      gptq:
#        gptq_act_order: false
#        gptq_ckpt: null
#        gptq_groupsize: -1
#        gptq_wbits: 16
    vllm: # see: https://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py
      block_size: 16
      disable_log_requests: false
      disable_log_stats: false
      download_dir: null
      dtype: auto
      engine_use_ray: false
      gpu_memory_utilization: 0.9
      load_format: auto
      max_log_len: null
      max_model_len: null
      max_num_batched_tokens: null
      max_num_seqs: 256
      max_paddings: 256
      pipeline_parallel_size: 1
      quantization: null
      revision: null
      seed: 0
      swap_space: 4
      tensor_parallel_size: 1
      tokenizer_mode: auto
      tokenizer_revision: null
      worker_use_ray: false
      max_parallel_loading_workers: null
      enable_prefix_caching: false
      enforce_eager: false
      max_context_len_to_capture: 8192
      kv_cache_dtype: 'auto'
      max_logprobs: 5  # OpenAI default value
      code_revision: null
      disable_custom_all_reduce: false
      enable_lora: false
      max_loras: 1
      max_lora_rank: 16
      lora_extra_vocab_size: 256
      lora_dtype: 'auto'
      max_cpu_loras: null
      ray_workers_use_nsight: false
  default_model_cfg:
    host: 127.0.0.1
    port: 20002
    device: cuda
    #infer_turbo: null
  model_cfg:
    Qwen-7B-Chat:
      infer_turbo: vllm
      path: /ai/models/Qwen-7B-Chat
      base:
        #num_gpus: 3
        gpus: "1,2,3,4"
        max_gpu_memory: 12GiB
    Qwen1.5-7B-Chat:
      #infer_turbo: vllm
      path: /ai/models/Qwen1.5-7B-Chat
      base:
        #num_gpus: 4
        gpus: "0,1,2,3"
        max_gpu_memory: 12GiB
      vllm:
        #max_model_len: 6016
        gpu_memory_utilization: 0.85
        tensor_parallel_size: 4
    Qwen-1.8B-Chat:
      path: /ai/models/Qwen-1_8B-Chat-Int8
      #infer_turbo: vllm
      quantize: int8
      base:
        gpus: "1"
        max_gpu_memory: 10GiB
      vllm:
        gpu_memory_utilization: 0.5
        tensor_parallel_size: 1
    chatglm3-6b:
      infer_turbo: vllm
      path: /ai/models/chatglm3-6b
      base:
        gpus: 1,2,3,4,5
        max_gpu_memory: 4GiB
    chatglm3-6b-32k:
      #infer_turbo: vllm
      path: /ai/models/chatglm3-6b-32k
      base:
        #num_gpus: 3
        gpus: "0,1,2,3"
        max_gpu_memory: 16GiB
      vllm:
        gpu_memory_utilization: 0.9
        tensor_parallel_size: 4
    Chinese-Alpaca-2-7B:
      path: /ai/models/chinese-alpaca-2-7b-hf
      base:
        num_gpus: 5
        max_gpu_memory: 5GiB
    Chinese-Alpaca-2-13B:
      path: /ai/models/chinese-alpaca-2-13b-16k-hf
      base:
        num_gpus: 5
        max_gpu_memory: 6GiB
    Llama2-Chinese-13b-Chat:
      path: /ai/models/Llama2-Chinese-13b-Chat
      base:
        num_gpus: 5
        max_gpu_memory: 6GiB
    Qwen-14B-Chat:
      path: /ai/models/Qwen-14B-Chat-Int8
      infer_turbo: vllm
      quantize: int8
      base:
        num_gpus: 4
        max_gpu_memory: 10GiB
    Yi-34B-Chat-8bits:
      path: /ai/models/Yi-34B-Chat-8bits
      infer_turbo: vllm
      quantize: int8
      base:
        num_gpus: 5
        max_gpu_memory: 16GiB
    Qwen-72B-Chat-Int8:
      path: /ai/models/Qwen-72B-Chat-Int8
      infer_turbo: vllm
      quantize: int8
      base:
        num_gpus: 4
        max_gpu_memory: 22GiB


embed:
  device: cuda
reranker:
  top_n: 20
  model:
    bge-reranker-large: /ai/models/BAAI_bge-reranker-large
    bge-reranker-base: BAAI/bge-reranker-base
    bce-reranker-base_v1: G:/50-TEMP/models/embed/bce-reranker-base_v1

test-aa:
  key-bb: 555