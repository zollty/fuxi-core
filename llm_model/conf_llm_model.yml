llm:
  device: auto
  controller:
    host: 0.0.0.0
    port: 21001
    dispatch-method: shortest_queue
    #log_level: INFO
  openai_api_server:
    host: 127.0.0.1
    port: 20000
    controller_addr: http://localhost:21001
    api_keys: EMPTY
    #log_level: INFO
  worker:
    host: 127.0.0.1
    start_port: 21105
    #log_level: null
    #cross_domain: true
    base:
      controller_addr: http://localhost:21001
      #worker_address
      limit_worker_concurrency: 100
      conv_template: null
      no_register: false
      device: auto
      #num_gpus: 1
      max_gpu_memory: 10GB
      #model_model_path: null
      #model_names: null
      trust_remote_code: true
    plain: # see: fastchat\serve\model_worker.py
      debug: true
      #dtype: null
      #seed: null
      load_8bit: false
      stream_interval: 2
      cpu_offloading: false
      embed_in_truncate: false
#      awq:
#        awq_ckpt: null
#        awq_groupsize: -1
#        awq_wbits: 16
#      gptq:
#        gptq_act_order: false
#        gptq_ckpt: null
#        gptq_groupsize: -1
#        gptq_wbits: 16
    vllm: # see: https://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py
      block_size: 16
      disable_log_requests: false
      disable_log_stats: false
      download_dir: null
      dtype: auto
      engine_use_ray: false
      gpu_memory_utilization: 0.9
      load_format: auto
      max_log_len: null
      max_model_len: null
      max_num_batched_tokens: null
      max_num_seqs: 256
      max_paddings: 256
      pipeline_parallel_size: 1
      quantization: null
      revision: null
      seed: 0
      swap_space: 4
      tensor_parallel_size: 1
      tokenizer_mode: auto
      tokenizer_revision: null
      worker_use_ray: false
      max_parallel_loading_workers: null
      enable_prefix_caching: false
      enforce_eager: false
      max_context_len_to_capture: 8192
      kv_cache_dtype: 'auto'
      max_logprobs: 5  # OpenAI default value
      code_revision: null
      disable_custom_all_reduce: false
      enable_lora: false
      max_loras: 1
      max_lora_rank: 16
      lora_extra_vocab_size: 256
      lora_dtype: 'auto'
      max_cpu_loras: null
      ray_workers_use_nsight: false
  default_model_cfg:
    host: 127.0.0.1
    port: 20002
    device: cuda
    #infer_turbo: null
  model_cfg:
    Qwen-7B-Chat:
      infer_turbo: vllm
      path: /ai/models/Qwen-7B-Chat
      base:
        #num_gpus: 3
        gpus: "1,2,3,4"
        max_gpu_memory: 12GiB
    Qwen1.5-7B-Chat:
      infer_turbo: vllm
      path: /ai/models/Qwen1.5-7B-Chat
      base:
        #num_gpus: 4
        gpus: "2,3"
        max_gpu_memory: 12GiB
      vllm:
        #max_model_len: 6016
        gpu_memory_utilization: 0.10
        tensor_parallel_size: 2
    Qwen-1.8B-Chat:
      path: /ai/models/Qwen-1_8B-Chat-Int8
      #infer_turbo: vllm
      quantize: int8
      base:
        gpus: "1"
        max_gpu_memory: 10GiB
      vllm:
        gpu_memory_utilization: 0.5
        tensor_parallel_size: 1
    chatglm3-6b:
      infer_turbo: vllm
      path: /ai/models/chatglm3-6b
      base:
        gpus: 1,2,3,4,5
        max_gpu_memory: 4GiB
    chatglm3-6b-32k:
      #infer_turbo: vllm
      path: /ai/models/chatglm3-6b-32k
      base:
        #num_gpus: 3
        gpus: "0,1,2,3"
        max_gpu_memory: 16GiB
      vllm:
        gpu_memory_utilization: 0.9
        tensor_parallel_size: 4
    Chinese-Alpaca-2-7B:
      path: /ai/models/chinese-alpaca-2-7b-hf
      base:
        num_gpus: 5
        max_gpu_memory: 5GiB
    Chinese-Alpaca-2-13B:
      path: /ai/models/chinese-alpaca-2-13b-16k-hf
      base:
        num_gpus: 5
        max_gpu_memory: 6GiB
    Llama2-Chinese-13b-Chat:
      path: /ai/models/Llama2-Chinese-13b-Chat
      base:
        num_gpus: 5
        max_gpu_memory: 6GiB
    Qwen-14B-Chat:
      path: /ai/models/Qwen-14B-Chat-Int8
      infer_turbo: vllm
      quantize: int8
      base:
        num_gpus: 4
        max_gpu_memory: 10GiB
    Yi-34B-Chat-8bits:
      path: /ai/models/Yi-34B-Chat-8bits
      infer_turbo: vllm
      quantize: int8
      base:
        num_gpus: 5
        max_gpu_memory: 16GiB
    Qwen-72B-Chat-Int8:
      path: /ai/models/Qwen-72B-Chat-Int8
      infer_turbo: vllm
      quantize: int8
      base:
        num_gpus: 4
        max_gpu_memory: 22GiB

  online_model_cfg:
    openai-api:
      model_name: "gpt-4"
      api_base_url: "https://api.openai.com/v1"
      api_key: ""
      openai_proxy: ""
    qwen-api:
      provider: "QwenWorker"
      version: "qwen-max"  # 可选包括 "qwen-turbo", "qwen-plus"
      api_key: ""
      embed_model: "text-embedding-v1"  # embedding 模型名称

  embed_model_cfg:
    bce-embedding-base_v1: "/ai/models/bce-embedding-base_v1"
    nlp_corom_sentence-embedding_chinese-base: "/ai/models/nlp_corom_sentence-embedding_chinese-base"
    nlp_gte_sentence-embedding_chinese-large: "/ai/models/nlp_gte_sentence-embedding_chinese-large"
    bge-large-zh-v1.5: "/ai/models/BAAI_bge-large-zh-v1.5" # "BAAI/bge-large-zh-v1.5"
    bge-large-zh: "/ai/models/BAAI_bge-large-zh" # "BAAI/bge-large-zh" # "/--/embeddings/BAAI/bge-large-zh_Merge_Keywords_20231215_132654" # BAAI/bge-large-zh
    bge-reranker-large: "/ai/models/BAAI_bge-reranker-large"
    m3e-large: "/ai/models/moka-ai_m3e-large" # "moka-ai/m3e-large"
    text2vec-bge-large-chinese: "/ai/models/text2vec-bge-large-chinese" # "shibing624/text2vec-bge-large-chinese"
#    ernie-tiny: "nghuyong/ernie-3.0-nano-zh"
#    ernie-base: "nghuyong/ernie-3.0-base-zh"
#    text2vec-base: "shibing624/text2vec-base-chinese"
#    text2vec: "GanymedeNil/text2vec-large-chinese"
#    text2vec-paraphrase: "shibing624/text2vec-base-chinese-paraphrase"
#    text2vec-sentence: "shibing624/text2vec-base-chinese-sentence"
#    text2vec-multilingual: "shibing624/text2vec-base-multilingual"
#    m3e-small: "moka-ai/m3e-small"
#    m3e-base: "moka-ai/m3e-base"
#    bge-small-zh: "BAAI/bge-small-zh"
#    bge-base-zh: "BAAI/bge-base-zh"
#    bge-large-zh-noinstruct: "BAAI/bge-large-zh-noinstruct"
#    bge-base-zh-v1.5: "BAAI/bge-base-zh-v1.5"
#    bge-m3: "BAAI/bge-m3"
#    piccolo-base-zh: "sensenova/piccolo-base-zh"
#    piccolo-large-zh: "sensenova/piccolo-large-zh"
#    text-embedding-ada-002: "your OPENAI_API_KEY"

embed:
  device: cuda
reranker:
  top_n: 20
  model:
    bge-reranker-large: /ai/models/BAAI_bge-reranker-large
    bge-reranker-base: BAAI/bge-reranker-base
    bce-reranker-base_v1: G:/50-TEMP/models/embed/bce-reranker-base_v1

test-aa:
  key-bb: 555